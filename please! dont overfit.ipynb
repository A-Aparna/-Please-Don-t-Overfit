{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Please! Don't Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Reading Training data\n",
    "data_df=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c424d74df2c04404a88edb58e17c100ba0a4a57"
   },
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is a good possibility of overfitting due to 300 features but only 250 records to train.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e73cb3cde996531cba7f3211ba7ab17756be97e"
   },
   "outputs": [],
   "source": [
    "#Seperating the features and label\n",
    "X=data_df.iloc[:,2:].values\n",
    "y=data_df.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = y ,palette=\"Set2\")\n",
    "sns.set(font_scale=1.5)\n",
    "ax.set_xlabel(' ')\n",
    "ax.set_ylabel(' ')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,5)\n",
    "ax.set_ylim(top=300)\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(y)), (p.get_x()+ 0.3, p.get_height()))\n",
    "\n",
    "plt.title('Distribution of labels/outputs')\n",
    "plt.xlabel('Output Label')\n",
    "plt.ylabel('Frequency [%]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization to see the correlation between the different features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = data_df.corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "cmap = sns.diverging_palette(0, 1000, as_cmap=True)\n",
    "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n",
    "             cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8561314fbb5edc6352343c2841052edbc1e3d8e3"
   },
   "outputs": [],
   "source": [
    "#Check for null data points\n",
    "data_df.isnull().any().any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardizing the data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4009b65b1bdc2d36d79c6cf33b8da54d0ac2916",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stc=StandardScaler()\n",
    "X_scale=stc.fit_transform(X)\n",
    "#split test and train\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting data into Training and Validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc2da551dfd335d7c602bb432869b23810770b2c"
   },
   "outputs": [],
   "source": [
    "#stratified K fold method to split Y \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "586bcc7a2ccb65f0485616b167cf66d96b77d17e"
   },
   "outputs": [],
   "source": [
    "#function to calculate ROC score for various algos.Returns mean of 5 ROC scores.\n",
    "def out_cross_val(model,X,y):\n",
    "    score=[]\n",
    "    for i,(train,test) in enumerate(skf.split(X,y)):\n",
    "        X_train,X_test=X[train],X[test]\n",
    "        y_train,y_test=y[train],y[test]\n",
    "        model=model\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred=model.predict(X_test)\n",
    "        rocscore=roc_auc_score(y_test,y_pred) \n",
    "        score.append(rocscore)\n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(score), np.std(score)))   \n",
    "    \n",
    "    return np.mean(score), np.std(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LOGISTIC REGRESSION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_LR = LogisticRegression(solver='liblinear')\n",
    "log_score=out_cross_val(model_LR,X_scale,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_LR=confusion_matrix(y,model_LR.predict(X_scale))\n",
    "cm_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18401875af780fc6f316ecc962ba579c443ad91a"
   },
   "source": [
    "***SUPPORT VECTOR CLASSIFIER***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4646759b72bfd3c68c6fe62a8dfdd7bd0a7d6e84"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_svc=SVC(random_state=0,gamma='scale',probability=True)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_svc=[{'C':[1,5,10],'kernel':['rbf','linear']}]\n",
    "gs_svc=GridSearchCV(estimator = classifier_svc,\n",
    "                           param_grid = param_svc,\n",
    "                           scoring = 'roc_auc',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "gs_svc=gs_svc.fit(X_scale,y)\n",
    "print('the best score is:{}'.format(gs_svc.best_score_))\n",
    "print('the best parameters are:{}'.format(gs_svc.best_params_)) \n",
    "means = gs_svc.cv_results_['mean_test_score']\n",
    "stds = gs_svc.cv_results_['std_test_score']\n",
    "  Zdxfor mean, std, params in zip(means, stds, gs_svc.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "            % (mean, std * 2, params))\n",
    "classifier_svc=SVC(random_state=0,gamma='scale',probability=True,**gs_svc.best_params_)\n",
    "svc_score=out_cross_val(classifier_svc,X_scale,y)\n",
    "classifier_svc=classifier_svc.fit(X_scale,y)\n",
    "#target_svc=classifier_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_svc=confusion_matrix(y,classifier_svc.predict(X))\n",
    "cm_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above confusion matrix,there are ***0 cases of incorrect predictions***.Data is ***overfitted***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gradient Boosting Classifier*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "GBM_score=out_cross_val(gbm0, X_scale, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "param_gbm=[{'n_estimators':range(20,81,10),'max_depth':range(5,16,2)}]\n",
    "#gsearch_gbm = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1,min_samples_leaf=50,max_features='sqrt',subsample=0.8,random_state=10), \n",
    " #                                                              param_grid = param_gbm, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch_gbm = GridSearchCV(estimator = GradientBoostingClassifier(),param_grid = param_gbm)\n",
    "gsearch_gbm.fit(X_scale,y)\n",
    "print('the best score is:{}'.format(gsearch_gbm.best_score_))\n",
    "print('the best parameters are:{}'.format(gsearch_gbm.best_params_)) \n",
    "means = gsearch_gbm.cv_results_['mean_test_score']\n",
    "stds = gsearch_gbm.cv_results_['std_test_score']\n",
    "\n",
    "    \n",
    "gbm0 = GradientBoostingClassifier(random_state=10,**gsearch_gbm.best_params_)\n",
    "GBM_score=out_cross_val(gbm0, X_scale, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried tuning various hyper parameters with no much improvement in the ROC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier:** Modeling the dataset with the default parameters of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_RFC=RandomForestClassifier(n_estimators=1000, min_samples_leaf=25, max_features=0.5, n_jobs=-1, \n",
    "                                oob_score=True)\n",
    "score_RFC=out_cross_val(model_RFC,X_scale,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "821df7b2f5301b730addab98a2956f838310e671"
   },
   "source": [
    "**LASSO MODEL:**For overfitting with many features it's a good option to implement L1 regularization technique and calculate ROC score.<br> A technical detail to note here is that once the threshold has been set to 0.01(by trial and error),the L1 model has assigned a weigth of 0 some of the feature.Hence the shape of input dataset(X_transform) is reduced to (250,140) from (250,300) i.e. from 300 features Lasso has discarded 160 features y assigining them a weight of zero.The ROC score has definitely  improved from 0.77 to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43807519788111ae3d9bb9eb0cf8b8234a43b0ff"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "model_lasso = Lasso(alpha=0.01)\n",
    "lasso_score=out_cross_val(model_lasso,X_scale,y)\n",
    "# Set a minimum threshold of 0.01 by trial and error\n",
    "sfm_lasso = SelectFromModel(model_lasso, threshold=0.001)\n",
    "#sfm = SelectFromModel(clf)\n",
    "sfm_lasso.fit(X_scale, y)\n",
    "X_transform = sfm_lasso.transform(X)\n",
    "print(X_transform.shape)\n",
    "lasso_featurescore=out_cross_val(model_lasso,X_transform,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting box plot of all the scores calculated so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "scores_df = pd.DataFrame({'LogisticRegression': log_score})\n",
    "scores_df['SVC'] = svc_score\n",
    "#scores_df['Lasso Uni-sel'] = lasso_score_US\n",
    "scores_df['GBM'] = GBM_score\n",
    "scores_df['Lasso Regularization'] = lasso_featurescore\n",
    "scores_df['RandomForestClassifier'] = score_RFC\n",
    "\n",
    "sns.boxplot(data=scores_df);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** As expected the Lasso tool lived upto its name by internally assigning the zero weights to the features which are leading to overfitting of the data.The other feature selection techniques do not seem to have significant effect on the ROC score in this particular scenario.Even Lasso based univariate selection with 150 features also seem to give a decent result compared to other models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
